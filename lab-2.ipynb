{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Basics of popular Python libraries for AI**"
      ],
      "metadata": {
        "id": "pcOJDpHdVGQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data processing using Pandas...\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Loading data from CSV file (with replacement of \"/\" values to NaN values)...\n",
        "data = pd.read_csv(\"/report.csv\", na_values = \"/\")                                                                          \t\t\t\t\t\n",
        "\n",
        "# Reading first 5 rows...\n",
        "print(data.head(10))                                                                                                        \t\t\t\t\t\t \n",
        "\n",
        "# Reading last 10 rows...\n",
        "print(data.tail(10))        \n",
        "                               \n",
        "# Reading specific column of DataFrame object                                                          \t\t\t\t\t\t \n",
        "print(data[\"Ispit1\"])                                                                                                        \t\t\t\t\t\t \n",
        "print(data[\"Ispit2\"])\n",
        "\n",
        "# Reading rows with specific characteristic...\n",
        "print(data.loc[data[\"Prisustvo\"] == 0])     \n",
        "\n",
        "# Replacing values (this is only for informative purpose, these values are already replaced)...\n",
        "data.replace(\"/\", np.nan, inplace = True)                                                                                    \t\t\t\t\t \n",
        "\n",
        "# Reading specific columns of rows with specific characteristic...\n",
        "print(data.loc[data[\"Ocjena\"] > 7].loc[:, [\"Indeks\", \"UKUPNO\", \"Ocjena\"]])                                                 \t\t      \t \t \n",
        "\t\n",
        "# Deleting specific rows of DataFrame object...\n",
        "data.dropna(subset = [\"Ocjena\"], inplace = True)                                                                             \t\t\t\t\t \n",
        "\n",
        "# Adding new column (with specific values)...\n",
        "temporaryStorage = data.replace(np.nan, -1)\n",
        "data[\"Ispit1_final\"] = np.maximum(temporaryStorage[\"Ispit1\"], temporaryStorage[\"Ispit1_popravni\"]).replace(-1, np.nan)                               \n",
        "data[\"Ispit2_final\"] = np.maximum(temporaryStorage[\"Ispit2\"], temporaryStorage[\"Ispit2_popravni\"]).replace(-1, np.nan)\n",
        "\n",
        "# Deleting specific columns of DataFrame object...\n",
        "data.drop(columns = [\"Ispit1\", \"Ispit2\", \"Ispit1_popravni\", \"Ispit2_popravni\"], inplace = True)                              \t\t\t \n",
        "\n",
        "# Saving DataFrame object as CSV file...\n",
        "data.to_csv(\"report-update.csv\", sep = \";\")\n",
        "\n",
        "# Saving DataFrame object as Pickle file...                                                                                                                                                                     \n",
        "data.to_pickle(\"report-update.p\")                                                                                                                                                                                                                                      "
      ],
      "metadata": {
        "id": "7sgI2byLVkGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data normalization using Sklearn...\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"/report.csv\", na_values = \"/\")\n",
        "\n",
        "# Creating imputers for handling NaN values...\n",
        "simpleImputerMedian = SimpleImputer(strategy = \"median\")                                                                                               \n",
        "simpleImputerMean = SimpleImputer(strategy = \"mean\")             \n",
        "\n",
        "# Creating MinMax scaler...\n",
        "minMaxScaler = MinMaxScaler()                                                                              \t\t\t\t  \n",
        "\n",
        " # Replacing NaN values with \"median\" strategy...\n",
        "data[\"Ispit1\"] = simpleImputerMedian.fit_transform(data[\"Ispit1\"].values.reshape(-1, 1))                    \t\t \n",
        "data[\"Ispit2\"] = simpleImputerMedian.fit_transform(data[\"Ispit2\"].values.reshape(-1, 1))\n",
        "\n",
        " # Replacing NaN values with \"mean\" strategy...\n",
        "data[\"Ispit1_popravni\"] = simpleImputerMean.fit_transform(data[\"Ispit1_popravni\"].values.reshape(-1, 1))   \t \n",
        "data[\"Ispit2_popravni\"] = simpleImputerMean.fit_transform(data[\"Ispit2_popravni\"].values.reshape(-1, 1))\n",
        "\n",
        "# Normalizing values using Z-score normalization...\n",
        "data[\"Ispit1\"] = scale(data[\"Ispit1\"])                                                                      \t\t\t\t  \n",
        "data[\"Ispit2\"] = scale(data[\"Ispit2\"])\n",
        "\n",
        "# Normalizing values using MinMax scaler...\n",
        "data[\"Ispit1_popravni\"] = minMaxScaler.fit_transform(data[\"Ispit1_popravni\"].values.reshape(-1, 1))         \t\t \n",
        "data[\"Ispit2_popravni\"] = minMaxScaler.fit_transform(data[\"Ispit2_popravni\"].values.reshape(-1, 1))       \n",
        "\n",
        "# Other NaN values will be 0...\n",
        "data.replace(np.nan, 0, inplace = True)                                                                     \t\t\t\t \n",
        "\n",
        "# Deleting column with grades...\n",
        "grades = data[\"Ocjena\"]\n",
        "data.drop(columns = [\"Ocjena\"], inplace = True)                                                             \t\t\t\t  \n",
        "\n",
        "# Converting DataFrame object to NumPy array...\n",
        "gradesNumPyArray = grades.to_numpy()                                                                        \t\t\t\t  \n",
        "dataNumPyArray = data.to_numpy()\n",
        "\n",
        "# Preparing for classification training...\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(dataNumPyArray, gradesNumPyArray, test_size = 0.2)          \t\t  "
      ],
      "metadata": {
        "id": "WLOGkVgnsW6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification of Iris flower using Sklearn...\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "# Loading dataset...\n",
        "iris = load_iris()                                                                                 \t\t\t \n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "featureNames = iris.feature_names\n",
        "targetNames = iris.target_names\n",
        "\n",
        "print(f\"Feature names: {featureNames}\")\n",
        "print(f\"Target names: {targetNames}\")\n",
        "print(f\"Data examples: {x[:5]}\")\n",
        "\n",
        "# Preparing dataset for training and evaluation...\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.3, random_state = 1)            \t\n",
        "\n",
        "# Preparing k-Nearest Neighbor cassifier...\n",
        "knnClassifier = KNeighborsClassifier(n_neighbors = 3)                                               \t\t\n",
        "\n",
        "# Training...\n",
        "knnClassifier.fit(xTrain, yTrain)                                                                   \t\t\t\n",
        "\n",
        "# Testing...\n",
        "yPredicted = knnClassifier.predict(xTest)                                                           \t\t\n",
        "accuracy = metrics.accuracy_score(yTest, yPredicted)\n",
        "print(f\"Accuracy: {accuracy}\") "
      ],
      "metadata": {
        "id": "NS7jRui-xVOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep learning using Keras (MNIST dataset)...\n",
        "\n",
        "\n",
        "# Importing MNIST dataset...\n",
        "from keras.datasets import mnist                                                                                                                                       \n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# Preparing training and testing sets...\n",
        "(xTrain, yTrain), (xTest, yTest) = mnist.load_data()                                                                                                           \n",
        "\n",
        "print(f\"Training set dimensions: {xTrain.shape}\")\n",
        "print(f\"Training set number of elements: {xTrain.shape[0]}\")\n",
        "print(f\"Training set size (bytes): {xTrain.itemsize * xTrain.size}\")\n",
        "\n",
        "print(f\"Testing set dimensions: {xTest.shape}\")\n",
        "print(f\"Testing set number of elements: {xTest.shape[0]}\")\n",
        "print(f\"Testing set size (bytes): {xTest.itemsize * xTest.size}\")\n",
        "\n",
        "# Preparing data for artificial neural network...\n",
        "trainImages = xTrain.reshape((xTrain.shape[0], 28 * 28))                                                                                                \n",
        "trainImages = trainImages.astype(\"float32\") / 255\n",
        "\n",
        "testImages = xTest.reshape((xTest.shape[0], 28 * 28))              \n",
        "testImages = testImages.astype(\"float32\") / 255\n",
        "\n",
        "print(f\"Training set size (bytes): {trainImages.itemsize * trainImages.size}\")\n",
        "print(f\"Testing set size (bytes): {testImages.itemsize * testImages.size}\")\n",
        "\n",
        "# Creating neural network model...\n",
        "neuralNetworkModel = models.Sequential()                                                                                                                    \n",
        "neuralNetworkModel.add(layers.Dense(512, activation = \"relu\", input_shape = (28 * 28,)))                \n",
        "neuralNetworkModel.add(layers.Dense(10, activation = \"softmax\"))\n",
        "\n",
        "# Compiling model...\n",
        "neuralNetworkModel.compile(optimizer = \"rmsprop\", loss = \"categorical_crossentropy\", metrics =[\"accuracy\"])     \n",
        "\n",
        "# Preparing labels...  \n",
        "trainLabels = to_categorical(yTrain)                                                                                                                                                \n",
        "testLabels = to_categorical(yTest)\n",
        "\n",
        "# Neural network model training...\n",
        "neuralNetworkModel.fit(trainImages, trainLabels, epochs = 5, batch_size = 128)                                                        \n",
        "\n",
        "# Evaulating model...\n",
        "testLoss, testAccuracy = neuralNetworkModel.evaluate(testImages, testLabels)                                                        \n",
        "print(f\"Accuracy (on testing data): {testAccuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOTZRnMoaZfF",
        "outputId": "8f5eddde-166a-44f7-b262-64f2522ff5c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set dimensions: (60000, 28, 28)\n",
            "Training set number of elements: 60000\n",
            "Training set size (bytes): 47040000\n",
            "Testing set dimensions: (10000, 28, 28)\n",
            "Testing set number of elements: 10000\n",
            "Testing set size (bytes): 7840000\n",
            "Training set size (bytes): 188160000\n",
            "Testing set size (bytes): 31360000\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 4s 6ms/step - loss: 0.2674 - accuracy: 0.9212\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.1075 - accuracy: 0.9683\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0719 - accuracy: 0.9786\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0518 - accuracy: 0.9848\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0386 - accuracy: 0.9881\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0661 - accuracy: 0.9805\n",
            "Accuracy (on testing data): 0.9804999828338623\n"
          ]
        }
      ]
    }
  ]
}